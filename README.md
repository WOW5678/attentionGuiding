# Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding

by Shanshan Wang, Muyang Ma, Zhumin Chen, Zhaochun Ren, Huasheng Liang, Qiang Yan, Pengjie Ren
>>@article{wang2022paying,\
>>  title={Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding},\
>>  author={Wang, Shanshan and Chen, Zhumin and Ren, Zhaochun and Liang, Huasheng and Yan, Qiang and Ren, Pengjie},\
>>  journal={arXiv preprint arXiv:2204.02922},\
>>  year={2022}\
>>}

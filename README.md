# Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding

by Shanshan Wang, Muyang Ma, Zhumin Chen, Zhaochun Ren, Huasheng Liang, Qiang Yan, Pengjie Ren
